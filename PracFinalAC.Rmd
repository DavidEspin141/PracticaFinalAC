---
title: "Aprendizaje Computacional: Práctica Final 2024/2025"
author: "Pedro García Montoya, Mario Martínez Turpin y David Espín Jiménez"
output:
  html_document:
    theme: spacelab
    highlight: kate
    df_print: paged
    toc: yes
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc: yes
editor_options:
  markdown:
    wrap: 72
---

# Carga de la base de datos

Para comenzar, debemos cargar la base de datos con los datos propuestos, así
como comprobar que la hemos cargado de manera correcta en nuestro entorno.

```{r}
credit<- read.csv("credit+approval/crx.data", header=FALSE)
credit.trainIdx<-readRDS("credit.trainIdx.rds")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]
nrow(credit.Datos.Train)
nrow(credit.Datos.Test)
```

# Análisis DEA

Antes de ponernos a tratar con los datos y analizar en detalle las
variables como se pide en el enunciado, hemos accedido a la página web
de la base de datos pues esta nos ofrece bastante información sobre el
tipo de datos que estamos tratando.\

Esta base de datos trata con valores relacionados con aplicaciones de
tarjetas de crédito, es por ello por lo que las variables y sus
correspondientes valores están cifrados con valores sin signficado, por
la confidencialidad de datos.\

Para esta práctica vamos a trabajar con el **aprendizaje supervisado**
donde tenemos 15 predictores (V1-V15) y una variable de respuesta $y$
(V16).

Si comprobamos la estructura de nuestra base de datos y la analizamos,
podemos ver como los datos no se han importado de la manera correcta.
Esto lo podemos ver comparando el tipo de dato ofrecido por el comando
`str(credit)` con la información de
\<<https://archive.ics.uci.edu/dataset/27/credit+approval>}

```{r}
str(credit)
```

Podemos ver como las variables categóricas
(V1,V4,V5,V6,V7,V9,V10,V12,V13 y V16) no se han importado como tales,
sino como caracteres (chr). 
Del mismo modo, V2 que es una variable numérica no tiene ese tipo de datos. 
Por tanto, lo primero que debemos de hacer antes de hacer ningún análisis de 
las variables es corregir esto.

```{r}
categoricos_col<-c("V1","V4","V5","V6","V7","V9","V10","V12","V13","V16")
credit[categoricos_col]<-lapply(credit[categoricos_col],FUN = as.factor)
str(credit)
```

Tras aplicar la transformación en los datos para convertirlos en
categóricos, podemos apreciar que algunos de ellos poseen más niveles de
los que aparecen en la página web, esto se debe a que hay algunos que
tienen valores incompletos, por los que también los considera como
nivel. 

A parte, podemos apreciar que la variable V4, posee 3 niveles (si
descontamos el nivel resultante de los valores incompletos), por lo que
según la información que hay en la página web, nos faltaría un nivel.
Podemos llegar a la conclusión de que este es un valor que forma parte
del dominio, para este nivel no hay ningún valor que aparezca en los
datos.

A continuación, vamos a observar de mejor forma la división de niveles
con el comando `levels()`.

```{r}
levels(credit$V1)
levels(credit$V4)
levels(credit$V5)
levels(credit$V6)
levels(credit$V7)
```

Con esto apreciamos de mejor forma que las variables categóricas que
tienen valores nulos son V1,V4,V5,V6 y V7.

Como hemos analizado, nuestra base de datos cuenta con datos missing,
por lo que sería conveniente cargarla de nuevo, pero indicándolo, para
que así se guarden directamente como datos NA y se pueda hacer un
tratamiento correcto de estos datos luego. 

Además, se hará todo el tratamiento anterior para dejar la base de datos como 
nos interesa, con los tipos de datos bien establecidos y los niveles de los 
mismos.

```{r}
rm(list = ls())
credit<- read.csv("credit+approval/crx.data", header=FALSE,na.strings = "?")
credit.trainIdx<-readRDS("credit.trainIdx.rds")
categoricos_col<-c("V1","V4","V5","V6","V7","V9","V10","V12","V13","V16")
credit[categoricos_col]<-lapply(credit[categoricos_col],FUN = as.factor)
levels(credit$V4)<-c(levels(credit$V4),"t")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]
nrow(credit.Datos.Train)
nrow(credit.Datos.Test)
```

Una vez hechos los cambios anteriores y antes de pasar a hacer un
análisis monovariable de algunas de las variables, vamos a ver de manera
resumida que información aporta cada variable, así como la cantidad de
datos desconocidos que tiene.

```{r}
summary(credit)
```

Como vemos, nuestros datos cuentan con NAs que habrá que solucionar para
poder hacer un buen modelo a posterior.\

Antes de tratar los NAs, debemos de tratar los datos atípicos que puedan
aparecer en nuestras variables, pues por ejemplo para las numéricas
estos valores atípicos influyen en la representación de su distribución
y por lo tanto podríamos elegir la media para sustituir los NAs cuando
quizás lo prudente sería la mediana.

```{r}
library(gridExtra)
library(ggplot2)
g1<-ggplot(na.omit(credit), aes(x = "", y = V2)) + 
  geom_boxplot() +
  labs(y = "V2", x = "") +
  theme_minimal()
g2<-ggplot(na.omit(credit), aes(x = "", y = V3)) + 
  geom_boxplot() +
  labs(y = "V3", x = "") +
  theme_minimal()
g3<-ggplot(na.omit(credit), aes(x = "", y = V8)) + 
  geom_boxplot() +
  labs(y = "V8", x = "") +
  theme_minimal()
g4<-ggplot(na.omit(credit), aes(x = "", y = V11)) + 
  geom_boxplot() +
  labs(y = "V11", x = "") +
  theme_minimal()
g5<-ggplot(na.omit(credit), aes(x = "", y = V14)) + 
  geom_boxplot() +
  labs(y = "V14", x = "") +
  theme_minimal()
g6<-ggplot(na.omit(credit), aes(x = "", y = V15)) + 
  geom_boxplot() +
  labs(y = "V15", x = "") +
  theme_minimal()
grid.arrange(g1,g2,g3,g4,ncol=2)
grid.arrange(g5,g6)
```

Como vemos, todas las variables numéricas cuentan con outliers. Estos
valores pueden resultar o no un problema, dependiendo del modelo que se
quiera idear, pues algunos como la regresión lineal tienen poca
tolerancia a outliers y otros como randomForest los toleran y tratan de
manera interna.\

Es por ello por lo que esto datos atípicos se solucionarán o no a
posteriori, cuando se haya elegido el modelo.

```{r}
(credit)[!complete.cases(credit),]
colSums(is.na(credit))

```

Con esto podemos ver que, en total contamos con 37 filas de las 690
observaciones totales que forman nuestro `data.frame` que cuentan con
NAs en alguna de sus columnas (predictores). 

Son las variables V1,V2,V4,V5,V6,V7 y V14 las que cuentan con NAs. 
El número de NAs es bastante pequeño y muy posiblemente lo más fácil sería 
directamente eliminar las observaciones que cuentan con NAs de nuestras
observaciones. 
Estas decisiones se tomarán más adelante en función del modelo usado y los 
requerimientos del mismo.

## Análisis monovariable

A continuación, vamos a llevar a cabo el análisis monovariable de
algunas de las variables del conjunto de datos. Para llevar a cabo esto,
el estudio lo vamos a realizar en variables o predictores de distinto
tipo, tanto en alguna númerica como en otra categórica.

En primera instanacia, vamos a comenzar por realizar un estudio
monovariable del predictor "V1", que se trata de un tipo de datos
categórico como hemos visto previamente. De todas formas, vamos a volver
a apreciarlo con porcentajes:

```{r}
porcent  <- prop.table(table(credit$V1)) * 100
cbind(total=table(credit$V1), porcentaje=porcent)
```

En este caso, al tener un tipo de datos categórico, no nos encontramos
con ningún dato fuera de rango o outlier, ya que los datos están
contenidos en el dominio de los dos niveles disponibles, "a" y "b".

Al no ser de tipo numérico tampoco podemos apreciar una gráfica con la
distribución de los datos y ver a que distribución se aproxima.

A continuación, vamos a analizar el predictor "V2" a fondo, que al
contrario que "V1", este sí que cuenta con datos númericos por lo que
podemos llevar a cabo un estudio más exhaustivo. Comenzamos de igual
forma que antes, apreciando un breve resuemen de la variable a
investigar:

```{r}
summary(credit$V2)
```

En este resumen, apreciamos que la mediana y la media tienen valores un
poco distantes, ya que el valor de la media es mayor debido a los
outliers por encima de la mediana que se encuentran en nuestro data
frame. Debemos de recordar que la mediana es un valor mucho más robusto
y menos suceptible a los outliers que la media, ya que esta última varía
más ante datos atípicos.

Además, en este caso podemos apreciar por medio de un histograma la
distribución que sigue este predictor:

```{r}
#Representamos histograma
myhist = ggplot(data=credit,aes(V2)) +
  geom_histogram(col="orange",fill="orange",alpha=0.2,) + 
  labs(title="Histograma V2", y="Count") 
#Marca el valor de la media con una línea azul vertical
myhist = myhist + geom_vline(xintercept = mean(credit$V2),
                             col="blue")
#Marca el valor de la mediana con una línea roja
myhist = myhist + geom_vline(xintercept = median(credit$V2),
                             col="red")
myhist+geom_rug(data=credit,aes(x=V2,y=0),
           sides="b",position="jitter")
```

Mediante el histograma resultante, podemos apreciar que efectivamente
como hemos mencionado antes, se trata de una distribución normal, aunque
en esta podemos apreciar muy claramente que se encuentra sesgada a la
derecha, debido a los outliers que se encuentran entre los datos. Debido
a la misma razón,ocurre lo mencionado con la media y la mediana.\

Los datos atípicos los identificamos mejor también gracias al comado
`jitter`, que crea un poco de ruido para poder visualizar de mejor forma
los valores de los datos repetidos (comprobamos que hay ciertos posibles
outliers en la parte derecha del histograma).

Por último vamos a analizar también el predictor "V3", cuyos datos
también son de tipo numérico. Como siempre, empezamos viendo un pequeño
resumen de los datos de la variable:

```{r}
summary(credit$V3)
```

En este caso, apreciamos que al igual que antes, la media y la mediana
difieren bastante entre sí, lo que vuelve a sugerir que haya outliers
entre los datos. A continuación, vamos a apreciar el histograma para
corrobar esta idea:

```{r}
#Representamos histograma
myhist = ggplot(data=credit,aes(V3)) +
  geom_histogram(col="orange",fill="orange",alpha=0.2,) + 
  labs(title="Histograma V3", y="Count") 
#Marca el valor de la media con una línea azul vertical
myhist = myhist + geom_vline(xintercept = mean(credit$V3),
                             col="blue")
#Marca el valor de la mediana con una línea roja
myhist = myhist + geom_vline(xintercept = median(credit$V3),
                             col="red")
myhist+geom_rug(data=credit,aes(x=V3,y=0),
           sides="b",position="jitter")
```

Gracias a `rug` de nuevo, apreciamos que también hay ciertos outliers
entre los datos, además de que el histograma resultante sigue una
distribución exponencial inversa, esto lo sabemos debido a la gran cola
que posee el histograma.

## Análisis multivariable

Por otro lado, vamos a llevar a cabo un estudio multivariable de los
predictores de la base de datos. Para el este, sería muy útil hacer uso
de un análisis de componentes principales, también denominado PCA, sin
embargo, se sabe que este tipo de análisis es muy susceptible a los
outliers, ya que se basa en la varianza, por lo que al todavía no ser
tratados, el PCA es inviable.

Primero, vamos a representar una matriz de correlación mediante el
comando `cor`, lo que nos va a permitir observar que tan relacionados
están entre sí los diversos predictores.

Después vamos a representar mediante el comando `pairs` de forma gráfica
las relaciones entre pares de predictores. A parte, vamos a
representarlo diferenciando los datos entre los diversos valores que
tienen según cada predictor de tipo categórico.

```{r}
cor(credit[, c("V2", "V3", "V8", "V11", "V14", "V15")])
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V1))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V4))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V5))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V6))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V7))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V9))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V10))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V12))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V13))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V16))
```

Tras observar la matriz resultante, podemos observar que no hay una gran
relación entre predictores aunque, entre ellos destacan la relación
entre "V2" y "V8" con un una correlación moderada de 0.39146361; la de
"V8" y "V11", con otra correlación de 0.32232967; y la relación entre
"V2" y "V3" de 0.20217658.

De entre las gráficas, no podemos sacar ninguna relación clara entre
tipos de datos, ya que los datos de los distintos niveles se encuentran
muy juntos. En las gráficas podemos apreciar que las relaciones
previamente mencionadas tienen una forma que, en algunos tramos, simula
una diagonal, lo que es muestra de su mayor nivel de correlación.

# Modelo Rpart

Una vez hecho el análisis previo, se va a comenzar a crear modelos de
predicción basados en aprendizaje supervisado.  

El modelo simple que vamos a usar en la práctica es Rpart (Recursive
Partitioning), ya que es una técnica rápida, fácil de interpretar,
eficaz y que trabaja con datos categóricos y numéricos, lo que
simplifica el preprocesamiento de estos. 

Para poder usar el modelo previamente debemos conocer sus
hiper-parámetros, para ello usamos `modelLookup()`

```{r}
library(caret)
modelLookup(("rpart"))
```

Vemos que el único hiper-parámetro de este modelo es **cp**, que indica
la complejidad del modelo, es decir, controla cuando el algoritmo crece
o para de dividirse.  

Antes de comenzar a entrenar tenemos que separar los predictores de la
variable objetivo.  

```{r}
credit.varSalida = c("V16")
credit.varsEntrada = setdiff(names(credit), credit.varSalida)
```

Vamos a hacer distintas pruebas con distintos preprocesamientos de datos
para encontrar el mejor resultado posible.  

Para comenzar, lo único que haremos es tratar los NAs de los datos de
entrenamiento para evitar posibles errores ya que `rpart` no procesa
correctamente dichos datos. 

Para ello, vamos a investigar en nuestras variables numéricas para ver
quédistribución siguen para así decidir si es mejor opción sustituir los
NAs por media o mediana. 

```{r}
library(ggplot2)
library(gridExtra)
p1 = ggplot(data=credit,aes(sample=V2)) +
  ggtitle("QQ plot para V2") +
  geom_qq() + 
  stat_qq_line() + 
  xlab("Distribución teórica") + ylab("Distribución muestral")
p2 = ggplot(data=credit,aes(sample=V14)) +
  ggtitle("QQ plot para V14") +
  geom_qq() + 
  stat_qq_line() + 
  xlab("Distribución teórica") + ylab("Distribución muestral")

grid.arrange(p1,p2,nrow=2)
```

Como podemos ver en las dos variables numéricas que contienen NAs, V2
sigue una normal cuyos valores se separan de esta en los extremos; en el
caso de V14, vemos que está sesgada a la derecha, así que decidimos
sustituir los NAs de estas variables por la mediana, ya que consideramos
una opción más sólida que la media, ya que esta podría estar desviada
debido a los valores altos.  

Por otro lado, las variables categóricas tendrán un tratamiento similar,
pero esta vez los NAs los asociaremos a la moda.

Lo primero que vamos a hacer, antes de hacer ningún tratamiento es
copiar los datos de la base de datos en una nueva variable, de modo que
podamos usar los datos en cada uno de los modelos como queramos, sin
necesidad de tener que estar cargando la base de datos todo el rato y
perdiendo las variables y modelos calculados.

```{r}
#Variables numéricas
credit.Datos.Train$V2[is.na(credit.Datos.Train$V2)] =
  median(credit.Datos.Train$V2,na.rm = TRUE)
credit.Datos.Train$V14[is.na(credit.Datos.Train$V14)] =
  median(credit.Datos.Train$V14,na.rm = TRUE)

#Variables categóricas
moda = 
  function(x) {
    names(which.max(table(x, useNA = "no")))  
  }
credit.Datos.Train$V1[is.na(credit.Datos.Train$V1)] = 
  moda(credit.Datos.Train$V1)
credit.Datos.Train$V4[is.na(credit.Datos.Train$V4)] = 
  moda(credit.Datos.Train$V4)
credit.Datos.Train$V5[is.na(credit.Datos.Train$V5)] = 
  moda(credit.Datos.Train$V5)
credit.Datos.Train$V6[is.na(credit.Datos.Train$V6)] = 
  moda(credit.Datos.Train$V6)
credit.Datos.Train$V7[is.na(credit.Datos.Train$V7)] = 
  moda(credit.Datos.Train$V7)

credit.Datos.Train.TratamientoOUT <- credit.Datos.Train

```

Una vez eliminados los NAs de nuestras variables, podemos comenzar a
hacer pruebas con el modelo.  

La primera prueba que haremos será entrenar el modelo sin preprocesar
más los datos, para ver los resultados con los que partimos en este
entrenamiento.  

```{r}
library(rpart)

set.seed(1234)
credit.modelo.rpart.noPreProc = train(credit.Datos.Train[credit.varsEntrada],
                            credit.Datos.Train[[credit.varSalida]],
                            method='rpart',
                            trControl = trainControl(method = "cv", number = 5))
credit.modelo.rpart.noPreProc
```

Con este entrenamiento podemos ver que, al estudiar la exactitud
(Accuracy),nos da valores superiores e inferiores a 85% dependiendo del
**cp** que se ha usado. 

Ahora vamos a probar tratando los datos atípicos para ver si podemos
mejorar el resultado obtenido previamente.

```{r}
#Primero debemos eliminar los outliers
eliminar_outliers = function(df, cols) {
  rows_to_keep = rep(TRUE, nrow(df))
  
  for (col in cols) {
    Q1 = quantile(df[[col]], 0.25, na.rm = TRUE)
    Q3 = quantile(df[[col]], 0.75, na.rm = TRUE)
    InterQ = Q3 - Q1
    
    lower_bound = Q1-1.5*InterQ
    upper_bound = Q3+1.5*InterQ
    
    rows_to_keep = rows_to_keep&(df[[col]] >= lower_bound&df[[col]] 
                                 <= upper_bound)
  }
  
  return(na.omit(df[rows_to_keep, ]))
}

numeric_cols = credit.varsEntrada[sapply(credit.Datos.Train.TratamientoOUT[credit.varsEntrada],
                                         is.numeric)]

credit.Datos.Train.TratamientoOUT = eliminar_outliers(credit.Datos.Train.TratamientoOUT, numeric_cols)

#Ya hemos tratado los datos, ahora volvemos a entrenar el modelo
set.seed(1234)
credit.modelo.rpart.PreProc = train(
              credit.Datos.Train.TratamientoOUT[credit.varsEntrada],
              credit.Datos.Train.TratamientoOUT[[credit.varSalida]],
              method = "rpart",
              trControl = trainControl(method = "cv", number = 5))
credit.modelo.rpart.PreProc

```

Ahora vamos a comparar los dos resultados obtenidos para ver cuál es más
preciso estudiando las predicciones realizadas por los modelos (tratando
datos y sin tratarlos) y haciendo matrices para comprobar la efectividad
de las pruebas. 

```{r}
pred_SinTratar = predict(credit.modelo.rpart.noPreProc, 
                          newdata = credit.Datos.Test[credit.varsEntrada])

confMatrix_SinTratar = confusionMatrix(pred_SinTratar, 
                                       credit.Datos.Test[[credit.varSalida]])

#Mostrar resultados
print(confMatrix_SinTratar)


pred_Tratado = predict(credit.modelo.rpart.PreProc, 
                       newdata = credit.Datos.Test[credit.varsEntrada])

confMatrix_Tratado = confusionMatrix(pred_Tratado, 
                                     credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrix_Tratado)

```

Como vemos en las matrices que mostramos, el modelo sin preprocesamiento
de datos tiene un valor de Accuracy superior al 85% mientras que este
valor en el modelo con los datos tratados es cercano al 84%.

# Modelo GBM

Uno de los modelos complejos que vamos a usar en esta práctica es GBM
(Stochastic Gradient Boosting).\ 

Antes de usar el modelo gbm es importante entender a este. Si hacemos
uso de `modelLookup()` podemos ver los hiper-parámetros con los que
trabaja este modelo.

```{r}
library(caret)
modelLookup(("gbm"))
```

1.  El hiper-parámetro **n.trees** controla cuántos árboles se entrenan.
    Más árboles suelen mejorar el ajuste, pero un exceso puede llevar al
    sobreajuste. (valor por defecto es 100). 
2.  El hiper-parámetro **interaction.depth** Determina la complejidad de
    los árboles. (valor 1= additive model; valor 2= 2way-directiones). 
3.  El hiper-parámetro **shrinkage** Controla cuánto contribuye cada
    árbol al modelo. Valores más bajos requieren más árboles para un
    buen ajuste. (valor por defecto es 0.1). 
4.  Finalmente, el hiper-parámetro **n.minobsinnode** Establece el
    tamaño mínimo de las hojas. Valores altos simplifican el modelo.
    (número real de observaciones, no el peso total).  Destacar que el
    modelo `gbm`tiene más hiper-parámetros que los `caret`pone a nuestra
    disposición. 

Una de las ventajas de `gbm`es que es un modelo capaz de tratar con
variables numéricas y categóricas.

Una vez hecho este análisis previo, ahora debemos de ajustar los datos
de la mejor manera para que este modelo trabaje de manera eficiente. 

`gbm`es un modelo que no maneja directamente los datos faltantes, por lo
que hay que tratarlos antes de hacer un entrenamiento del modelo.

Para tratar los datos nulos, se ha decidido sustituir estos valores
faltantes, pues no sabemos lo que signfican las variables, por lo que
eliminar las observaciones que contienen NAs no parece la mejor opción.

Como se ha visto en la sección anterior, las variables numéricas
conviene sustituirlas por la mediana como se ha hecho antes.

```{r}
library(gbm)
set.seed(1234)
modelo.gbm.SinTratarDatos <- train(
              x = credit.Datos.Train[credit.varsEntrada],
              y = credit.Datos.Train[[credit.varSalida]],
              method = "gbm",
              trControl = trainControl(method = "cv", number = 5),
              verbose = FALSE              
)
modelo.gbm.SinTratarDatos
```

Como el tratamiento de outliers sería el mismo que se ha realizado en el
modelo Rpart, lo mejor es usar el conjunto de datos que se ha creado
para representar que se han tratado los outliers.

```{r}
set.seed(1234)
modelo.gbm.TratarDatos <- train(
              x = credit.Datos.Train.TratamientoOUT[credit.varsEntrada],
              y = credit.Datos.Train.TratamientoOUT[[credit.varSalida]],
              method = "gbm",
              trControl = trainControl(method = "cv", number = 5),
              verbose = FALSE              
)
modelo.gbm.TratarDatos
```

Ahora, vamos a comparar el modelo gbm, tanto con tratamiento de outliers
como sin él, para ver que da mejores resultados.

```{r}
# Predicciones para el modelo sin tratar
pred_SinTratar <- predict(modelo.gbm.SinTratarDatos, newdata = credit.Datos.Test[credit.varsEntrada])

# Evaluar desempeño en el conjunto de prueba
confMatrix_SinTratar <- confusionMatrix(pred_SinTratar, credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrix_SinTratar)
# Predicciones para el modelo con tratamiento de outliers
pred_Tratar <- predict(modelo.gbm.TratarDatos, newdata = credit.Datos.Test[credit.varsEntrada])

# Evaluar desempeño en el conjunto de prueba
confMatrix_Tratar <- confusionMatrix(pred_Tratar, credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrix_Tratar)

```
Como se puede ver, el modelo con el tratamiento de outliers ofrece un accuracy superior al modelo sin tratar estos datos. \ 
Ahora, vamos a explorar hiper-parámetros a trabes de la parrilla de
valores, para tratar de buscar una configuración óptima que obtenga
mejores resultados.

```{r}
tuneGridBuscarGbm = expand.grid(
  n.trees = seq(100, 1000, by = 100),  # Número de árboles
  interaction.depth = c(1, 3, 5),      # Profundidad del árbol
  shrinkage = c(0.01, 0.1, 0.2),       # Tasa de aprendizaje
  n.minobsinnode = c(10, 20, 30)       # Tamaño mínimo del nodo
)
set.seed(1234)
modelo.gbm.BuscarHiper <- train(
              x = credit.Datos.Train.TratamientoOUT[credit.varsEntrada],
              y = credit.Datos.Train.TratamientoOUT[[credit.varSalida]],
              method = "gbm",
              trControl = trainControl(method = "cv", number = 5),
              tuneGrid = tuneGridBuscarGbm,
              verbose = FALSE              
)
modelo.gbm.BuscarHiper
```
Una vez que se ha hecho este modelo con el tuneGrid, vamos a crear otro que no explore hiper-parámetros y tan solo use los valores que acaban de ser seleccionados como mejores. \ 
Los resultados que se obtienen con el ajuste de hiper-parámetros son:

```{r}
tuneGridMejoresHiper <- expand.grid(
  n.trees = 200,
  interaction.depth = 1,
  shrinkage = 0.1,
  n.minobsinnode = 30
)

set.seed(1234)
modelo.gbm.Hiper <- train(
              x = credit.Datos.Train.TratamientoOUT[credit.varsEntrada],
              y = credit.Datos.Train.TratamientoOUT[[credit.varSalida]],
              method = "gbm",
              trControl = trainControl(method = "cv", number = 5),
              tuneGrid = tuneGridMejoresHiper,
              verbose = FALSE              
)
modelo.gbm.Hiper
# Predicciones para el modelo sin tratar
predHiper <- predict(modelo.gbm.Hiper, newdata = credit.Datos.Test[credit.varsEntrada])

# Evaluar desempeño en el conjunto de prueba
confMatrixHiper <- confusionMatrix(predHiper, credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrixHiper)

```

# Modelo RF

Ahora vamos a hacer uso de RF (Random Forest), otro de los modelos
complejos a utilizar.

Antes de empezar vamos a volver a hacer uso de `modelLookup()` con el
fin de apreciar los hiperparámetros.

```{r}
library(caret)
modelLookup("rf")
```

En este caso en concreto, apreciamos que rf solo cuenta con un parámetro
en concreto con el que podemos hacer pruebas, este es **mtry**. En este
parámetro se refleja el número de predictores o características que son
seleccionados aleatoriamente en cada nodo de árbol para llevar a cabo la
división necesaria o "split".

Hay otro parámetro intersante que no se ha usado (debido a los malos
resultados obtenidos a partir de él), este es **ntree**, que es el
número concreto de árboles a generar en el random forest, aunque
obteníamos mejores resultados dejando el valor por defecto proporcionado
por caret(500).

Al igual que `gbm` previamente, `rf` también es capaz de tratar tanto
con variables o predictores numéricos como categóricos. Por otro lado,
también cuenta con una gran robustez ante datos atípicos (información de
la cual haremos uso en cuanto al tratamiento previo de los datos) y al
denominado "overfitting"o sobrentrenamiento.

A continuación, vamos a llevar a cabo el tratamiento de los datos previo
a las pruebas. A pesar de su gran robustez ante outliers, `rf` no puede
trabajar con datos missing, por lo que vamos a tratar con el mismo
tratamiento inicial que el resto de modelos previamente mencionados, es decir, imputar los NAs con la mediana (aunque también podríamos por la media debido a la gran robustez del modelo frente a outliers) en el caso de los datos númericos y de la moda en el caso de los categóricos.

Para empezar vamos a llevar a cabo diversas pruebas. Las primeras las
vamos a llevar a cabo simplemente eliminando los outliers.

```{r}
library(randomForest)
#primera prueba (haciendo que caret internamente, encuentre los mejores valores de mtry)
modelo.rf.SinTratarDatosV1<- train(
  x = credit.Datos.Train[credit.varsEntrada],
  y = credit.Datos.Train[[credit.varSalida]],
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance=TRUE,
  verbose = FALSE
)

imp1 <- varImp(modelo.rf.SinTratarDatosV1, scale = FALSE)

# Graficar la importancia de los predictores
plot(imp1,main="Importancia en el primer modelo eliminando NAs")

print(modelo.rf.SinTratarDatosV1)

#segunda prueba (haciendo uso de tunegrid probando todos los valores de mtry psodibles)
set.seed(1234)
rf.tunegrid <- expand.grid(mtry = 1:15)

modelo.rf.SinTratarDatosV2<- train(
  x = credit.Datos.Train[credit.varsEntrada],
  y = credit.Datos.Train[[credit.varSalida]],
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid=rf.tunegrid,
  importance=TRUE,
  verbose = FALSE
)

imp2 <- varImp(modelo.rf.SinTratarDatosV2, scale = FALSE)

# Graficar la importancia de los predictores
plot(imp2,main="Importancia en el segundo modelo eliminando NAs")

print(modelo.rf.SinTratarDatosV2)
#tercera prueba (haciendo uso de un valor fijo de mtry)

set.seed(1234)
modelo.rf.SinTratarDatosV3 <- train(
  x = credit.Datos.Train[credit.varsEntrada],
  y = credit.Datos.Train[[credit.varSalida]],
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(mtry = 1),
  importance=TRUE,
  verbose = FALSE
)

imp3 <- varImp(modelo.rf.SinTratarDatosV1, scale = FALSE)

# Graficar la importancia de los predictores
plot(imp3,main="Importancia en el tercer modelo eliminando NAs")

print(modelo.rf.SinTratarDatosV3)


```

Cabe mencionar que de estas pruebas, la prueba del valor fijo, hemos
hecho pruebas con todos los valores fijos posibles de mtry. Sin embargo,
el mejor resultado lo obtenemos a posteriori con 'mtry=1'.

En la salida apreciamos diversas gráficas que muestran los diverosos
pesos o importancia que ha tenido cada predictor durante la creación del
modelo. Además, observamos también los resultados en el índice de certeza de los distintos modelos. En el primer modelo, obtenemos un índice del 87% con los valores de mtry 2, 8, 15 (se supone que son los valores con los que caret ha encontrado un mejor rendimiento, aunque finalmente destaca el 8 entre ellos); en el segundo, el mejor resultado ronda el 88% (caret vuelve a decir que se ha quedado con el valor de mtry de 8); mientras que en el último modelo explorado, se obtiene un índice del 85,88%. 

Ahora apreciando los valores obtenidos a partir del parámetro
**importance**, vemos que hay ciertos predictores en algunos modelos que
tienen importancia negativa (V12 en el primer y tercer modelo, lo que se supone que influye de forma negativa en el resultado del modelo), sin embargo haciendo diversas pruebas borrando estos, no obtenemos mejores resultados. 

Ahora vamos a hacer un tratamiento diferente, en el que en vez de
tratar solo los datos missing, vamos a aplicar el mismo tratamiento
anteriormente usado en modelos anteriores, es decir borrar los outliers.

```{r}
set.seed(1234)
modelo.rf.TratarDatos <- train(
  x = credit.Datos.Train.TratamientoOUT[credit.varsEntrada],
  y = credit.Datos.Train.TratamientoOUT[[credit.varSalida]],
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  verbose = FALSE
)

print(modelo.rf.TratarDatos)
```

En este caso, no se ha realizado un estudio de las variables o
predictores con mayor importancia, ya que al usarlo, se obtenía unos
resultados peores. Observamos por otro lado que el valor de certeza del
entrenamiento en este caso ronda el 85,7% (caret escoge el valor de mtry 2). 

Por último, vamos a observar el resultado que obtenemos en los
diferentes modelos finales, llevando a cabo la predicción con los datos
del conjunto de "test".\ 

Primero de los modelos eliminando NAs.

```{r}
# Predicciones para el modelos sin tratamiento
set.seed(1234)
pred_rf.SinTratarV1 <- predict(modelo.rf.SinTratarDatosV1, newdata = credit.Datos.Test[credit.varsEntrada])

set.seed(1234)
pred_rf.SinTratarV2 <- predict(modelo.rf.SinTratarDatosV2, newdata = credit.Datos.Test[credit.varsEntrada])

set.seed(1234)
pred_rf.SinTratarV3 <- predict(modelo.rf.SinTratarDatosV3, newdata = credit.Datos.Test[credit.varsEntrada])

# Evaluar desempeño en el conjunto de prueba
confMatrix_rf.SinTratar1 <- confusionMatrix(pred_rf.SinTratarV1, credit.Datos.Test[[credit.varSalida]])
confMatrix_rf.SinTratar2 <- confusionMatrix(pred_rf.SinTratarV2, credit.Datos.Test[[credit.varSalida]])
confMatrix_rf.SinTratar3 <- confusionMatrix(pred_rf.SinTratarV3, credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrix_rf.SinTratar1)
print(confMatrix_rf.SinTratar2)
print(confMatrix_rf.SinTratar3)
```

En cuanto a estos modelos solo tratando NAs, obtenemos sólo un modelo
satisfactorio, que es el modelo usando un valor fijo del hiperparámetro
"mtry", en este caso 1. Obtenemos un resultado de certeza del 88.32%, lo
que refleja un muy buen índice de eficacia. 

Por otra parte, vamos a observar el rendimiento obtenido por el modelo
haciendo uso de un tratamiento de NAs y outliers.

```{r}
# Predicciones para el modelo con tratamiento
set.seed(1234)
pred_rf.Tratar <- predict(modelo.rf.TratarDatos, newdata = credit.Datos.Test[credit.varsEntrada])

# Evaluar desempeño en el conjunto de prueba
confMatrix_rf.Tratar <- confusionMatrix(pred_rf.Tratar, credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrix_rf.Tratar)

```

Observamos que, con el modelo tratando los datos NAs y los outliers,
obtenemos un índice de certeza del 86,86%, lo que resulta un índice
bastante aceptable ya que es mayor del 85%.

# Modelo NNET

Para trabajar con el modelo **nnet**, que representa una red neuronal,
primero debemos estudiar el modelo, viendo los hiper-parámetros que
contiene.

```{r}
modelLookup("nnet")
```

Vemos que **nnet** tiene 2 hiper-parámetros, size y decay, por lo que
podremos modificarlos en un futuro.  

-**size** representa el tamaño de la capa oculta de la red, es decir el número de neuronas que tiene esta. 

-**decay** representa la tasa de decaimiento de la regularización L2. 

Para empezar con el entrenamiento, debemos saber que el modelo nnet no procesa datos nulos, es decir, el modelo dará errores si lo entrenamos sin hacer una modificación previa de los datos que tenemos.  

Como sabemos que las variables numéricas con valores nulos son V2 y V14,
las procesamos sustituyendo los valores nulos por el valor de la mediana
para esa variable.\ 

Para las variables categóricas hacemos lo mismo pero aplicando los
valores nulos a la moda de cada variable.\ 

Tras eliminar los valores nulos ya podemos entrenar el modelo. En este
hemos usado el parámetro preProcess ya que nnet es muy sensible a las
escalas de los valores de entrada, y trace = FALSE por no llenar el
terminal de información en cada iteración que se haga en el
entrenamiento. 

```{r}
set.seed(1234)
# Entrenando el modelo de red neuronal
tuneGrid <- expand.grid(
  #size = c(1, 2, 5),  # Número de neuronas en la capa oculta
  size = seq(1,10,1),
  decay = c(0.001, 0.01, 0.1,0.5)  # Tasa de regularización L2
)
credit.modelo.nnet <- train(
  credit.Datos.Train.TratamientoOUT[credit.varsEntrada],    # Datos de entrada
  credit.Datos.Train.TratamientoOUT[[credit.varSalida]],    # Variable de salida
  preProcess = c("center", "scale"),  # Preprocesamiento de los datos
  method = "nnet",                            # Usando redes neuronales
  tuneGrid = tuneGrid,                        # Usando la parrilla de parámetros
  maxit = 20,                               # Número máximo de iteraciones
  trace = FALSE,                              # No mostrar el progreso
  skip=TRUE
)
# Ver los resultados del modelo
print(credit.modelo.nnet)

```
Por otra parte, cabe mencionar también el uso del parámetro **maxit** durante el entrenamiento.Este parámetro representa el número máximo de iteraciones para el proceso de ajuste de los parámetros de la red neuronal (pesos).

En el entrenamiento observamos que caret encuentra la combinación de hiperparámetros de (size=0.6 y decay 0.5) como la óptima, la cual obtiene un índice de certeza del 84.47%.

A continuación, vamos a observar el rendimiento/índice de certeza del modelo aplicando predicciones en este con el conjunto test de datos.
```{r}
#Mostrar resultados

pred_red_neuronal = predict(credit.modelo.nnet, 
                       newdata = credit.Datos.Test[credit.varsEntrada])

confMatrix = confusionMatrix(pred_red_neuronal, 
                                     credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados
print(confMatrix)

```
Finalmente, observamos que el modelo final obtiene un ínidice de certeza del 86,86% al comprobar con el conjunto de datos test.\ 

# Comparación de modelos

Una vez que ya hemos tratado diferentes modelos para el mismo conjunto de datos y hemos analizado los resultados que ofrecen, estamos en disposición de analizar las mejores versiones de `gbm`, `rpart`, `rf` y `nnet`para quedarnos finalmente con un modelo para nuestro aprendizaje supervisado. 
```{r}

library(pROC)
# Obtener las probabilidades para cada modelo
prob_rpart <- predict(credit.modelo.rpart.noPreProc, credit.Datos.Train, type = "prob")[, 2]
prob_rf <- predict(modelo.rf.SinTratarDatosV3, credit.Datos.Train, type = "prob")[, 2]
prob_gbm <- predict(modelo.gbm.Hiper, credit.Datos.Train.TratamientoOUT, type = "prob")[, 2]
prob_nnet <- predict(credit.modelo.nnet, credit.Datos.Train.TratamientoOUT, type = "prob")[, 2]

# Calcular ROC y AUC
roc_rpart <- roc(credit.Datos.Train$V16, prob_rpart)
roc_gbm <- roc(credit.Datos.Train.TratamientoOUT$V16, prob_gbm)
roc_rf <- roc(credit.Datos.Train$V16, prob_rf)
roc_nnet <- roc(credit.Datos.Train.TratamientoOUT$V16, prob_nnet)

# AUC
auc_rpart <- auc(roc_rpart)
auc_gbm <- auc(roc_gbm)
auc_rf <- auc(roc_rf)
auc_nnet <- auc(roc_nnet)

# Mostrar AUC
print(auc_rpart)
print(auc_gbm)
print(auc_rf)
print(auc_nnet)

# Graficar la curva ROC
plot(roc_rpart, col = "blue", main = "Curva ROC")
plot(roc_gbm, col = "red", add = TRUE)
plot(roc_rf, col = "green", add = TRUE)
plot(roc_nnet, col = "purple", add = TRUE)
legend("bottomright", legend = c("rpart", "gbm", "rf", "nnet"), col = c("blue", "red", "green", "purple"), lwd = 2)

```

Si analizamos en detalle las salidas de las curvas ROC para cada uno de los 4 modelos, podemos ver como la relación entre los verdaderos positivos y las falsas alarmas es mucho mejor en `randomForest`, pues la que más cercana a 1 se queda. \ 
Estos valores son algo que esperábamos, pues ya en las pruebas anteriores habíamos visto como randomForest era el que mejor **accuracy** obtenía con los datos de test. \ 

# Elección del modelo final

Nuestra decisión final es elegir el modelo de `randomForest` llamado `modelo.rf.SinTratarDatosV3`como nuestro modelo final para este proyecto.\  
Esto se debe a que este modelo ha sido el que mejores resultados ha obtenido en accuracy, algo muy importante ya que aquí consigue unos valores muy buenos en nuestro conjunto de datos de test. 

Es decir, nuestro modelo generaliza bastante bien para casos que no han sido incluidos en su entrenamiento. Por otro lado, es el que más cerca a 1 se queda en su representación a través de una curva ROC. \ 

Aquí se muestra, de manera exclusiva, los resultados que obtiene este modelo:
```{r}
set.seed(1234)
modeloFinal <- modelo.rf.SinTratarDatosV3
predFinal <- predict(modeloFinal, newdata = credit.Datos.Test[credit.varsEntrada])

# Evaluar desempeño en el conjunto de prueba

confMatrixFinal <- confusionMatrix(predFinal, credit.Datos.Test[[credit.varSalida]])

# Mostrar resultados

print(confMatrixFinal)
```
