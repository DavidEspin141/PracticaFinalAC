---
title: "Aprendizaje Computacional: Práctica Final 2024/2025"
author: "Pedro García Montoya, Mario Martínez Turpin y David Espín Jiménez"
output:
  html_document:
    theme: spacelab
    highlight: kate
    df_print: paged
    toc: yes
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc: yes
editor_options:
  markdown:
    wrap: 72
---

# Carga de la base de datos

Con esto comprobamos que hemos cargado de manera correcta la base de
datos.

```{r}
credit<- read.csv("credit+approval/crx.data", header=FALSE)
credit.trainIdx<-readRDS("credit.trainIdx.rds")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]
nrow(credit.Datos.Train)
nrow(credit.Datos.Test)
```

# Sección 1: Análisis DEA

Antes de ponernos a tratar con los datos y analizar en detalle las
variables como se pide en el enunciado, hemos accedido a la página web
de la base de datos pues esta nos ofrece bastante información sobre el
tipo de datos que estamos tratando.\
Esta base de datos trata con valores relacionados con aplicaciones de
tarjetas de crédito, es por ello por lo que las variables y sus
correspondientes valores están cifrados con valores sin signficado, por
la confidencialidad de datos.\
Para esta práctica vamos a trabajar con el **aprendizaje supervisado**
donde tenemos 15 predictores (V1-V15) y una variable de respuesta $y$
(V16).

Si comprobamos la estructura de nuestra base de datos y la analizamos,
podemos ver como los datos no se han importado de la manera correcta.
Esto lo podemos ver comparando el tipo de dato ofrecido por el comando
`str(credit)` con la información de
\<<https://archive.ics.uci.edu/dataset/27/credit+approval>}

```{r}
str(credit)
```

Podemos ver como las variables categóricas
(V1,V4,V5,V6,V7,V9,V10,V12,V13 y V16) no se han importado como tales,
sino como caracteres (chr). Del mismo modo, V2 que es una variable
numérica no tiene ese tipo de datos. Por tanto, lo primero que debemos
de hacer antes de hacer ningún análisis de las variables es corregir
esto.

```{r}
categoricos_col<-c("V1","V4","V5","V6","V7","V9","V10","V12","V13","V16")
credit[categoricos_col]<-lapply(credit[categoricos_col],FUN = as.factor)
str(credit)
```

Tras aplicar la transformación en los datos para convertirlos en
categóricos, podemos apreciar que algunos de ellos poseen más niveles de
los que aparecen en la página web, esto se debe a que hay algunos que
tienen valores incompletos, por los que también los considera como
nivel. A parte, podemos apreciar que la variable V4, posee 3 niveles (si
descontamos el nivel resultante de los valores incompletos), por lo que
según la información que hay en la página web, nos faltaría un nivel.
Podemos llegar a la conclusión de que este, es un valor que forma parte
del dominio, para este nivel no hay ningún valor que aparezca en los
datos.

A continuación, vamos a observar de mejor forma la división de niveles
con el comando `levels()`.

```{r}
levels(credit$V1)
levels(credit$V4)
levels(credit$V5)
levels(credit$V6)
levels(credit$V7)


```

Con esto apreciamos de mejor forma, que las variables categóricas que
tienen valores nulos son V1,V4,V5,V6 y V7.

Como hemos analizado, nuestra base de datos cuenta con datos missing,
por lo que sería conveniente cargarla de nuevo, pero indicándolo, para
que así se guarden directamente como datos NA y se pueda hacer un
tratamiento correcto de estos datos luego.

```{r}
rm(list = ls())
credit<- read.csv("credit+approval/crx.data", header=FALSE,na.strings = "?")
credit.trainIdx<-readRDS("credit.trainIdx.rds")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]
nrow(credit.Datos.Train)
nrow(credit.Datos.Test)
```

Una vez hecho esto, hay que aplicar todos los pasos anteriores para
dejar las variables con su tipo correspondiente.

```{r}
categoricos_col<-c("V1","V4","V5","V6","V7","V9","V10","V12","V13","V16")
credit[categoricos_col]<-lapply(credit[categoricos_col],FUN = as.factor)
str(credit)
```

Además, como hemos mencionado antes, en la variable V4 vemos como no
aparece el nivel "t".\
Esto se puede añadir facilmente de la siguiente manera:

```{r}
levels(credit$V4)<-c(levels(credit$V4),"t")
str(credit$V4)
```

Una vez hechos los cambios anteriores y antes de pasar a hacer un
análisis monovariable de algunas de las variables, vamos a ver de manera
resumida que información aporta cada variable, así como la cantidad de
datos desconocidos que tiene.

```{r}
summary(credit)
```

Como vemos, nuestros datos cuentan con NAs que habrá que solucionar para
poder hacer un buen modelo a posterior.\
Antes de tratar los NAs, debemos de tratar los datos atípicos que puedan
aparecer en nuestras variables, pues por ejemplo para las numéricas
estos valores atípicos influyen en la representación de su distribución
y por lo tanto podríamos elegir la media para sustituir los NAs cuando
quizás lo prudente sería la mediana.

```{r}
library(gridExtra)
library(ggplot2)
g1<-ggplot(na.omit(credit), aes(x = "", y = V2)) + 
  geom_boxplot() +
  labs(y = "V2", x = "") +
  theme_minimal()
g2<-ggplot(na.omit(credit), aes(x = "", y = V3)) + 
  geom_boxplot() +
  labs(y = "V3", x = "") +
  theme_minimal()
g3<-ggplot(na.omit(credit), aes(x = "", y = V8)) + 
  geom_boxplot() +
  labs(y = "V8", x = "") +
  theme_minimal()
g4<-ggplot(na.omit(credit), aes(x = "", y = V11)) + 
  geom_boxplot() +
  labs(y = "V11", x = "") +
  theme_minimal()
g5<-ggplot(na.omit(credit), aes(x = "", y = V14)) + 
  geom_boxplot() +
  labs(y = "V14", x = "") +
  theme_minimal()
g6<-ggplot(na.omit(credit), aes(x = "", y = V15)) + 
  geom_boxplot() +
  labs(y = "V15", x = "") +
  theme_minimal()
grid.arrange(g1,g2,g3,g4,ncol=2)
grid.arrange(g5,g6)
```

Como vemos, todas las variables numéricas cuentan con outliers. Estos
valores pueden resultar o no un problema, dependiendo del modelo que se
quiera idear, pues algunos como la regresión lineal tienen poca
tolerancia a outliers y otros como randomForest los toleran y tratan de
manera interna.\
Es por ello por lo que esto datos atípicos se solucionarán o no a
posteriori, cuando se haya elegido el modelo.

```{r}
(credit)[!complete.cases(credit),]
colSums(is.na(credit))

```

Con esto podemos ver, que en total contamos con 37 filas de las 690
observaciones totales que forman nuestro `data.frame` que cuentan con
NAs en alguna de sus columnas (predictores). Son las variables
V1,V2,V4,V5,V6,V7 y V14 las que cuentan con NAs. El número de NAs es
bastante pequeño y muy posiblemente lo más fácil sería directamente
eliminar las observaciones que cuentan con NAs de nuestras
observaciones. Sin embargo, al no saber el contexto de los datos ni lo
que signfica cada variable, hemos decidido hacer un tratamiento más
conservador.\
Para ello, lo que se va a hacer va a ser sustituir el valor de NA por el
más conveniente según el tipo de dato y la distribución que sigue.

```{r}
#Comprobación visual para determinar si los variables numéricas se ajustan a una normal
library(ggplot2)
library(gridExtra)
p1 = ggplot(data=credit,aes(sample=V2)) +
  ggtitle("QQ plot para V2") +
  geom_qq() + 
  stat_qq_line() + 
  xlab("Distribución teórica") + ylab("Distribución muestral")
p2 = ggplot(data=credit,aes(sample=V14)) +
  ggtitle("QQ plot para V14") +
  geom_qq() + 
  stat_qq_line() + 
  xlab("Distribución teórica") + ylab("Distribución muestral")

grid.arrange(p1,p2,nrow=2)
```

Si analizamos la distribución que siguen las dos variables numéricas que
presentan NAs, podemos ver como V2 sigue una normal que se separa un
poco en los extremos. Por otro lado, V14 si que está sesgada hacia la
derecha. Es por ello, por lo que para estas variables se ha decidido
imputar los NAs con la mediana, pues es una medida robusta a los
outliers ya que la media puede estar bastante desviada debido a valores
extremos altos.

```{r}
credit$V2[is.na(credit$V2)] <-median(credit$V2,na.rm = TRUE)
credit$V14[is.na(credit$V14)] <-median(credit$V14,na.rm = TRUE)
```

Ahora, para las variables categóricas el procedimiento que hemos
decidido seguir es el de sustituir los NAs por la moda, ya que el número
total de NAs en las columnas es muy pequeño y con esto conseguimos
mantener la distribución que siguen.

```{r}
# Función para calcular la moda
moda <- function(x) {
  names(which.max(table(x, useNA = "no")))  
}
credit$V1[is.na(credit$V1)] <- moda(credit$V1)
credit$V4[is.na(credit$V4)] <- moda(credit$V4)
credit$V5[is.na(credit$V5)] <- moda(credit$V5)
credit$V6[is.na(credit$V6)] <- moda(credit$V6)
credit$V7[is.na(credit$V7)] <- moda(credit$V7)
summary(credit)
```

Con esto, hemos conseguido tratar los NAs, de manera que ahora tratar
con las variables será mucho más fácil y no aparecerán warnings en
gráficas.

## Análisis monovariable

A continuación, vamos a llevar a cabo el análisis monovariable de
algunas de las variables del conjunto de datos. Para llevar a cabo esto,
el estudio lo vamos a realizar en variables o predictores de distinto
tipo, tanto en alguna númerica como en otra categórica.

En primera instanacia, vamos a comenzar por realizar un estudio
monovariable del predictor "V1", que se trata de un tipo de datos
categórico como hemos visto previamente. De todas formas, vamos a volver
a apreciarlo con porcentajes:

```{r}
porcent  <- prop.table(table(credit$V1)) * 100
cbind(total=table(credit$V1), porcentaje=porcent)
```

En este caso, al tener un tipo de datos categórico, no nos encontramos
con ningún dato fuera de rango o outlayer, ya que los datos están
contenidos en el dominio de los dos niveles disponibles, "a" y "b".

Al no ser de tipo númerico tampoco podemos apreciar una gráfica con la
distribución de los datos y ver a que distribución se aproxima.

A continuación, vamos a analizar el predictor "V2" a fondo, que al
contrario que "V1", este sí que cuenta con datos númericos por lo que
podemos llevar a cabo un estudio más exhaustivo. Comenzamos de igual
forma que antes, apreciando un breve resuemen de la variable a
investigar:

```{r}
summary(credit$V2)
```

En este resumen, apreciamos que la mediana y la media tienen valores un
poco distantes, ya que el valor de la media es mayor debido a los
outliers por encima de la mediana que se encuentran en nuestro data
frame. Debemos de recordar que la mediana es un valor mucho más robusto
y menos suceptible a los outliers que la media, ya que esta última varía
más ante datos atípicos.

Además, en este caso podemos apreciar por medio de un histograma la
distribución que sigue este predictor:

```{r}
#Representamos histograma
myhist = ggplot(data=credit,aes(V2)) +
  geom_histogram(col="orange",fill="orange",alpha=0.2,) + 
  labs(title="Histograma V2", y="Count") 
#Marca el valor de la media con una línea azul vertical
myhist = myhist + geom_vline(xintercept = mean(credit$V2),
                             col="blue")
#Marca el valor de la mediana con una línea roja
myhist = myhist + geom_vline(xintercept = median(credit$V2),
                             col="red")
myhist+geom_rug(data=credit,aes(x=V2,y=0),
           sides="b",position="jitter")
```

Mediante el histograma resultante, podemos apreciar que efectivamente
como hemos mencionado antes, se trata de una distribución normal, aunque
esta podemos apreciar muy claramente que se encuentra sesgada a la
derecha, debido a los outlyers que se encuentran entre los datos. Debido
a la misma razón,ocurre lo mencionado con la media y la mediana.\
Los datos atípicos, los identificamos mejor también gracias al comado
`jitter`, que crea un poco de ruido para poder visualizar de mejor forma
los valores de los datos repetidos (comprobamos que hay ciertos posibles
outliers en la parte derecha del histograma).

Por último vamos a analizar también el predictor "V3", cuyos datos
también son de tipo numérico. Como siempre, empezamos viendo un pequeño
resumen de los datos de la variable:

```{r}
summary(credit$V3)
```

En este caso, apreciamos que al igual que antes, la media y la mediana
difieren bastante entre sí, lo que vuelve a sugerir que haya outliers
entre los datos. A continuación, vamos a apreciar el histograma para
corrobar esta idea:

```{r}
#Representamos histograma
myhist = ggplot(data=credit,aes(V3)) +
  geom_histogram(col="orange",fill="orange",alpha=0.2,) + 
  labs(title="Histograma V3", y="Count") 
#Marca el valor de la media con una línea azul vertical
myhist = myhist + geom_vline(xintercept = mean(credit$V3),
                             col="blue")
#Marca el valor de la mediana con una línea roja
myhist = myhist + geom_vline(xintercept = median(credit$V3),
                             col="red")
myhist+geom_rug(data=credit,aes(x=V3,y=0),
           sides="b",position="jitter")
```

Gracias a `rug` de nuevo, apreciamos que también hay ciertos outliers
entre los datos, además de que el histograma resultante sigue una
distribución exponencial inversa, esto lo sabemos debido a la gran cola
que posee el histograma.

## Análisis multivariable

Por otro lado, vamos a llevar a cabo un estudio multivariable de los
predictores de la base de datos. Para el este, sería muy útil hacer uso
de un análisis de componentes principales, también denominado PCA, sin
embargo, se sabe que este tipo de análisis es muy susceptible a los
outliers, ya que se basa en la varianza, por lo que al todavía no ser
tratados, el PCA es inviable.

Primero, vamos a representar una matriz de correlación mediante el
comando `cor`, lo que nos va a permitir observar que tan relacionados
están entre sí los diversos predictores.

Después vamos a representar mediante el comando `pairs` de forma gráfica
las relaciones entre pares de predictores. A parte, vamos a
representarlo diferenciando los datos entre los diversos valores que
tienen según cada predictor de tipo categórico.

```{r}
cor(credit[, c("V2", "V3", "V8", "V11", "V14", "V15")])
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V1))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V4))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V5))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V6))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V7))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V9))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V10))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V12))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V13))
pairs(credit[,c("V2","V3","V8","V11","V14","V15")],col=as.numeric(credit$V16))
```

Tras observar la matriz resultante, podemos observar que no hay una gran
relación entre predictores aunque, entre ellos destacan la relación
entre "V2" y "V8" con un una correlación moderada de 0.39146361; la de
"V8" y "V11", con otra correlación de 0.32232967; y la relación entre
"V2" y "V3" de 0.20217658.

De entre las gráficas, no podemos sacar ninguna relación clara entre
tipos de datos, ya que los datos de los distintos niveles se encuentran
muy juntos. En las gráficas podemos apreciar que, las relaciones
previamente mencionadas, tienen una forma que en algunos tramos, simula
una diagonal, lo que es muestra de su mayor nivel de correlación.


## Seccion 2

En esta sección vamos a entrenar el modelo con los datos dados, para ello
lo primero que debemos hacer es elegir las técnicas de entrenamiento según
las ofrecidas por caret.

En nuestro caso hemos elegido usar rpart (árboles de decisión) como técnica
de tipo sencillo, y svmRadial (Support Vector Machine), gbm (Gradient 
Boosting Machine) y rf (Bosques Aleatorios) como técnicas complejas.

Antes de comenzar a entrenar tenemos que separar los predictores de la 
variable objetivo.

```{r}
credit.varSalida = c("V16")
credit.varsEntrada = setdiff(names(credit), credit.varSalida)
```

Para empezar, entrenaremos el modelo con rpart, vemos que no es necesario
ningún preprocesamiento especial ya que tolera las variables categóricas
y no le influye la escala.

```{r}
credit.modelo.rpart = train(credit.Datos.Train[credit.varsEntrada],
                            credit.Datos.Train[[credit.varSalida]],
                            method='rpart')
credit.modelo.rpart
```
Ahora ajustaré eso 